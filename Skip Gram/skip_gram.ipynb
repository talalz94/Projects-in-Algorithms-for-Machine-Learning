{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import progressbar\n",
    "#matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 10\n",
    "EMBEDDING_DIM = 200\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"your_file.txt\"\n",
    "with open(filename, \"rb\") as file:\n",
    "        raw_text = file.read().decode(\"utf-8\").strip()\n",
    "#raw_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function for reading a file and converting into list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    with open(filename, \"rb\") as file:\n",
    "        processed_text = word_tokenize(file.read().decode(\"utf-8\").strip())\n",
    "    return processed_text\n",
    "processed_text = read_file(filename)\n",
    "#processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_processing(processed_text):\n",
    "    vocab = {}\n",
    "    ix_to_word = {}\n",
    "    word_to_ix = {}\n",
    "    total = 0.0\n",
    "    for word in processed_text:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 0\n",
    "            ix_to_word[len(word_to_ix)] = word\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "        vocab[word] += 1.0\n",
    "        total += 1.0\n",
    "    return vocab, ix_to_word, word_to_ix\n",
    "vocab,ix_to_word,word_to_ix = mini_processing(processed_text)\n",
    "#print(\"Vocaublary:\", vocab)\n",
    "#print(\"ix_to_word\",ix_to_word)\n",
    "#print(\"word_to_ix\",word_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generating training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gather_training_data(processed_text, word_to_ix, context_size):\n",
    "    training_data = []\n",
    "    for i, word in enumerate(processed_text):\n",
    "        back_i = i - 1\n",
    "        back_c = 0\n",
    "        forw_i = i + 1\n",
    "        forw_c = 0\n",
    "        while (back_i >= 0 and back_c < context_size):\n",
    "            training_data.append(([word_to_ix[word]], word_to_ix[processed_text[back_i]]))\n",
    "            back_i -= 1\n",
    "            back_c += 1\n",
    "        while (forw_i < len(processed_text) and forw_c < context_size):\n",
    "            training_data.append(([word_to_ix[word]], word_to_ix[processed_text[forw_i]]))\n",
    "            forw_i += 1\n",
    "            forw_c += 1\n",
    "    return training_data\n",
    "training_data = gather_training_data(processed_text, word_to_ix, CONTEXT_SIZE)\n",
    "#training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = self.linear(embeds)\n",
    "        log_probs = F.log_softmax(out)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkipGram(len(vocab),EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Loss and optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NEGLoss(nn.Module):\n",
    "    def __init__(self, ix_to_word, word_freqs, num_negative_samples=5,):\n",
    "        super(NEGLoss, self).__init__()\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "        self.num_words = len(ix_to_word)\n",
    "        self.distr = F.normalize(torch.Tensor(\n",
    "            [word_freqs[ix_to_word[i]] for i in range(len(word_freqs))]).pow(0.75), dim=0\n",
    "        )\n",
    "\n",
    "    def sample(self, num_samples, positives=[]):\n",
    "        weights = torch.zeros((self.num_words, 1))\n",
    "        for w in positives: weights[w] += 1.0\n",
    "        for _ in range(num_samples):\n",
    "            w = torch.multinomial(self.distr, 1)[0]\n",
    "            while (w in positives):\n",
    "                w = torch.multinomial(self.distr, 1)[0]\n",
    "            weights[w] += 1.0\n",
    "        return weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        #print(\"target= \",target)\n",
    "        #print(\"sample\", self.sample(self.num_negative_samples, positives=target.data.numpy()))\n",
    "        #print(\"input = \",input)\n",
    "        #print(\"\\n\")\n",
    "        return F.nll_loss(input, target,\n",
    "            self.sample(self.num_negative_samples, positives=target.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = NEGLoss(ix_to_word,vocab)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses=[]\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    print(\"Beginning epoch %d\" % epoch)\n",
    "    progress_bar = progressbar.ProgressBar()\n",
    "    for context, target in progress_bar(training_data):\n",
    "        context_var = autograd.Variable(torch.LongTensor(context))\n",
    "        model.zero_grad()\n",
    "        log_probs = model(context_var)\n",
    "        #print(\"target\",target)\n",
    "        #print(\"log_probs\",log_probs)\n",
    "        #print('\\n')\n",
    "        loss = loss_function(log_probs, autograd.Variable(\n",
    "            torch.LongTensor([target])))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.data\n",
    "    print(\"Epoch %d Loss: %.5f\" % (epoch, total_loss[0]))\n",
    "    losses.append(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.choice(np.arange(len(vocab)), size=100, replace=True)\n",
    "\n",
    "## The comments are there to make obvious the pairs.\n",
    "\n",
    "#syns = [('chand','kuch'),('dhool','matti'),('australia','international'),('baarisho','water'),('workers','mazdoor'),\n",
    " #    ('diye','paish'),('mehnat','taraqqi'),('chairman','chancellor'),('officer','chairman'),('maqam','elaqon'),('senetor',\n",
    "    #'wazir'), ('zimmay','akhrajaat'),('kyunki','kyunkay'),('hamaray','hum'),('gaind','baal'),('London','Bartanwi')]\n",
    "\n",
    "syns = ['chand','kuch','dhool','matti','Australia','international','baarisho','water','workers','mazdoor',\n",
    "     'diye','paish','mehnat','taraqqi','chairman','chancellor','officer','chairman','maqam','elaqon','senetor','wazir',\n",
    "       'zimmay','akhrajaat','kyunki','kyunkay','hamaray','hum','gaind','baal','London','Bartanwi']\n",
    "\n",
    "#analogies = [('australia','international'),('water','atlantic'),('mayor','belgium'),('university','oxford'),\n",
    "#             ('darkhwast','article'),('pakhton','Afghan'),('relief','water'),('mumalik','belgium'),('mayor','himayat'),\n",
    "#             ('college','principal'),('news','ghalat'),('naam','Ahmed'),('wet','water')]\n",
    "\n",
    "analogies = ['Australia','international','water','atlantic','mayor','belgium','university','oxford',\n",
    "             'darkhwast','article','pakhton','Afghan','relief','water','mumalik','belgium','mayor','himayat',\n",
    "             'college','principal','naam','Ahmed']\n",
    "\n",
    "#misspelled = [('lout','out'),('elaqon','ilaqon'),('Diasel','diesel'),('kyunki','kyunkay'),('neh','nay','ne'),('hai','hain')]\n",
    "\n",
    "misspelled =['lout','out','elaqon','ilaqon','Diasel','diesel','kyunki','kyunkay','neh','nay','ne','hai','hain']\n",
    "\n",
    "\n",
    "#Change the variable 'misspelled' to 'analogies' or 'syns'\n",
    "\n",
    "for i in analogies:\n",
    "    word = i\n",
    "    input = autograd.Variable(torch.LongTensor([word_to_ix[word]]))\n",
    "    vec = model.embeddings(input).data[0]\n",
    "    x, y = vec[0], vec[1]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(word, xy=(x, y), xytext=(5, 2),textcoords='offset points', ha='right', va='bottom')\n",
    "    plt.savefig(\"graph.png\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
